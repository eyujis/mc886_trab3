{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "from sklearn import metrics\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "import sklearn as sk\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processamento dos dados\n",
    "**Carregando os dados de treino e teste:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = pd.read_csv('../fashion-mnist_train.csv')\n",
    "test_images = pd.read_csv('../fashion-mnist_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Separando as labels do conjuntos:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels=train_images.loc[:, ['label']]\n",
    "train_images=train_images.drop(['label'], axis=1)\n",
    "test_labels=test_images.loc[:, ['label']]\n",
    "test_images=test_images.drop(['label'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalizando os data-sets:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------\n",
    "## Construção do modelo base\n",
    "**Esta rede será utilizada posteriomente para comparações entre os modelos de dimensionalidade reduzida utilizando PCA e autoencoder.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definição da estrutura do modelo por meio do Keras Sequential:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 32)                4128      \n",
      "=================================================================\n",
      "Total params: 62,688\n",
      "Trainable params: 62,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(64,activation=tf.nn.relu,input_dim=(784)),\n",
    "    keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(32, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compilação do modelo:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Treino do modelo:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_images, train_labels, epochs=10, validation_split=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verificação dos resultados do modelo:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "10000/10000 [==============================] - 1s 53us/sample - loss: 0.3678 - accuracy: 0.8629\n",
      "Model - 3 layers - test loss: 36.77934731006622\n",
      "Model - 3 layers - test accuracy: 86.29000186920166\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(\"Model - 3 layers - test loss:\", test_loss * 100)\n",
    "print(\"Model - 3 layers - test accuracy:\", test_acc * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------\n",
    "## Redução de Dimensionalidade Usando Principal component analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessamento de dados (Feature Scaling)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(train_images)\n",
    "\n",
    "train_images_r = scaler.transform(train_images)\n",
    "test_images_r = scaler.transform(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definição do número de componentes:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Construção dos novos datasets de treino e de teste com dimensões reduzidas usando o PCA:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=3, random_state=None,\n",
       "    svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.fit(train_images_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_r = pca.transform(train_images_r)\n",
    "test_images_r = pca.transform(test_images_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessamento: normalização e alteração do tipo dos dados de numpy array para pandas dataframe:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#esse conjunto rr sera utilizado pela rede neural\n",
    "train_images_rr = normalize(train_images_r)\n",
    "test_images_rr = normalize(test_images_r)\n",
    "train_images_rr = pd.DataFrame(data=train_images_rr)\n",
    "test_images_rr = pd.DataFrame(data=test_images_rr)\n",
    "\n",
    "#esse conjunto rr sera utilizado posteriormente para a clusterizacao \n",
    "train_images_r = pd.DataFrame(data=train_images_r)\n",
    "test_images_r= pd.DataFrame(data=test_images_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definição da estrutura do modelo por meio do Keras Sequential que receberá os dados com dimensões reduzidas pelo PCA:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 32)                4128      \n",
      "=================================================================\n",
      "Total params: 12,704\n",
      "Trainable params: 12,704\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_r = keras.Sequential([\n",
    "    keras.layers.Dense(64,activation=tf.nn.relu,input_dim=(3)),\n",
    "    keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(32, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "model_r.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compilação do modelo:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_r.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalização dos dados e Treino do modelo:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 3s 73us/sample - loss: 1.1409 - accuracy: 0.5673 - val_loss: 1.0396 - val_accuracy: 0.5890\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 3s 61us/sample - loss: 1.0209 - accuracy: 0.5958 - val_loss: 1.0193 - val_accuracy: 0.5951\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 3s 56us/sample - loss: 1.0118 - accuracy: 0.5985 - val_loss: 1.0218 - val_accuracy: 0.5966\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 3s 57us/sample - loss: 1.0057 - accuracy: 0.6013 - val_loss: 1.0139 - val_accuracy: 0.6015\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 3s 68us/sample - loss: 1.0030 - accuracy: 0.6013 - val_loss: 1.0157 - val_accuracy: 0.5996\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 3s 64us/sample - loss: 1.0001 - accuracy: 0.6046 - val_loss: 1.0055 - val_accuracy: 0.5982\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 3s 59us/sample - loss: 0.9973 - accuracy: 0.6060 - val_loss: 1.0103 - val_accuracy: 0.5983\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 3s 57us/sample - loss: 0.9957 - accuracy: 0.6081 - val_loss: 1.0057 - val_accuracy: 0.6031\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 3s 62us/sample - loss: 0.9938 - accuracy: 0.6068 - val_loss: 1.0034 - val_accuracy: 0.6037\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 3s 64us/sample - loss: 0.9929 - accuracy: 0.6081 - val_loss: 1.0033 - val_accuracy: 0.6008\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x13da6cad0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_r.fit(train_images_rr, train_labels, epochs=10, validation_split=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verificação dos resultados do modelo:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "10000/10000 [==============================] - 0s 31us/sample - loss: 0.9940 - accuracy: 0.6128\n",
      "Model - 3 layers - test loss: 99.40232433319092\n",
      "Model - 3 layers - test accuracy: 61.28000020980835\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model_r.evaluate(test_images_rr, test_labels)\n",
    "print(\"Model - 3 layers - test loss:\", test_loss * 100)\n",
    "print(\"Model - 3 layers - test accuracy:\", test_acc * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------\n",
    "\n",
    "## Redução de Dimensionalidade Usando Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the size of our encoded representations\n",
    "encoding_dim = 2\n",
    "input_img = Input(shape=(784,))\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_img)\n",
    "encoder = Model(input_img, encoded)\n",
    "decoded = Dense(784, activation='sigmoid')(encoded)\n",
    "autoencoder = Model(input_img, decoded)\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a placeholder for an encoded (32-dimensional) input\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "# retrieve the last layer of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "# create the decoder model\n",
    "decoder = Model(encoded_input, decoder_layer(encoded_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REMOVER troquei o 'adadelta' pelo 'adam'\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.fit(train_images, train_images,epochs=10,shuffle=True,validation_data=(test_images, test_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_imgs_train = encoder.predict(train_images)\n",
    "encoded_imgs_test = encoder.predict(test_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_a = pd.DataFrame(data=encoded_imgs_train)\n",
    "test_images_a =pd.DataFrame(data=encoded_imgs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 64)                192       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 32)                4128      \n",
      "=================================================================\n",
      "Total params: 12,640\n",
      "Trainable params: 12,640\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_r = keras.Sequential([\n",
    "    keras.layers.Dense(64,activation=tf.nn.relu,input_dim=(2)),\n",
    "    keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(32, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "model_r.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_r.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 4s 79us/sample - loss: 1.2934 - accuracy: 0.4773 - val_loss: 1.1764 - val_accuracy: 0.5260\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 3s 71us/sample - loss: 1.1678 - accuracy: 0.5226 - val_loss: 1.1464 - val_accuracy: 0.5408\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 3s 70us/sample - loss: 1.1524 - accuracy: 0.5302 - val_loss: 1.1409 - val_accuracy: 0.5386\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 3s 67us/sample - loss: 1.1429 - accuracy: 0.5348 - val_loss: 1.1447 - val_accuracy: 0.5359\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 3s 66us/sample - loss: 1.1399 - accuracy: 0.5381 - val_loss: 1.1337 - val_accuracy: 0.5425\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 3s 67us/sample - loss: 1.1348 - accuracy: 0.5399 - val_loss: 1.1346 - val_accuracy: 0.5346\n",
      "Epoch 7/10\n",
      "28096/48000 [================>.............] - ETA: 1s - loss: 1.1354 - accuracy: 0.5410"
     ]
    }
   ],
   "source": [
    "model_r.fit(train_images_a, train_labels, epochs=10, validation_split=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model_r.evaluate(test_images_a, test_labels)\n",
    "print(\"Model - 3 layers - test loss:\", test_loss * 100)\n",
    "print(\"Model - 3 layers - test accuracy:\", test_acc * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------\n",
    "\n",
    "## Clustering com K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optamos pela utilização das imagens reduzidas por meio do PCA, já que obtiveram por meio deste modelo os melhores resultados de predição na rede neural.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_k = train_images_r\n",
    "test_images_k = test_images_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Construção do modelo do k-means com 10 clusters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmean_model = KMeans(n_clusters=10, random_state=10)\n",
    "y_km = kmean_model.fit(train_images_k)\n",
    "labels = kmean_model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_centroids = kmean_model.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_score(X = train_images_k, labels = labels, random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.v_measure_score(train_labels.values.ravel(), labels, beta=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------\n",
    "## Clustering com o DBScan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
